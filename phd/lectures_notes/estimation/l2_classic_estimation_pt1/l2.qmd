---
title: Lesson 2 - 
author: "Diogo Silva"
date: "2023-09-223"
last-modified: "2023-09-30"
categories: [estimation-classification-course,phd,lecture-notes]
draft: false
format:
  html:
    code-fold: true
---

# Overview

- Classic estimation
    - Parabolic fit
    - Least mean squares
    - Estimation
    - Regression
    - Frequency estimation
    - Robustness
    - RANSAC
    - Exercises & Work
- Classic probabilistic methods
    - classic vs bayesian
    - Maximum Likelihood
    - ML vs LS
    - Crámer-Rao bound
    - Monte-Carlo
    - Exercises
- Bibliography
    - Duda, Hart, Stork, Pattern Classification, Wiley, 2001.
    - J. Marques, Reconhecimento de Padrões. Métodos Estatísticos e Neuronais, IST Press, 1999


# Exercises

## Problem 1 - linear least squares, predict signal at t

Given a signal $y=( y_1 , ..., y_N )$, determine the coefficients of the linear predictor

$y_t = a_1 y_{t-1} +  ...  a_p y_{t-p}, N>>P$

using the least squares method.

---

Let $\theta = [a_1,..., a_p]^T$ be the parameter set.

Error is given by


$$
E = \sum_{i}{(y_i - x_i \theta )^2}
$$

Where, for each $y_i$, we have $x_i = (y_{i-1}, ..., y_{i-p})$. This means, using historical data, we can't use $y_1, ..., y_p$.

We want to minimize $\theta$ such that $E = 0$.

$$
\frac{\partial E}{\partial \theta_j} = \frac{\partial \sum_{i}{(y_i - x_i \theta )^2}}{\partial \theta_j} = 
2 \sum_{i}{ \frac{\partial(y_i - x_i \theta )}{\partial \theta_j}} = 
2 \sum_{i}{ -x_{i_j} \theta_j}
$$

We then have 

$$
\frac{\partial E}{\partial \theta} = 
\begin{bmatrix}
-2 \sum_{i}{x_{i_1} \theta_1} \\
\vdots \\
-2 \sum_{i}{x_{i_p} \theta_p} \\
 \end{bmatrix}
 = 0
$$

Solving this equation will give us the parameter values.


## Problem 2 - predict signal in [t,t+k] TODO RECHECK

By increasing the prediction window, we're increasing the number of parameters.
If, for each prediction, we were looking at $p$ previous signals and using $p$ parameters for prediciting the next signal $y_t$, now we'll need $k \times p$ parameters for predicting $(y_t, ..., y_{t+k})$. We have 



$$
\begin{bmatrix}
x_1 \\
\vdots \\
x_N
\end{bmatrix}
\times
\begin{bmatrix}
a_{1_1}  & ... & a_{k_1} \\
\vdots &  & \vdots \\
a_{1_p}  & ... & a_{k_p} \\
\end{bmatrix} = 
\begin{bmatrix}
y_{1_1} & ... & y_{1_k} \\
\vdots &  & \vdots \\
y_{N_1}  & ... & y_{N_k} \\
\end{bmatrix}
$$

Since the prediciton at $y_t$ doesn't affect subsequent predictions and the parameters for $y_t$

$$
\frac{\partial E}{\partial \theta} = 
\begin{bmatrix}
-2 \sum_{i}{x_{i_1} \theta_{1_1}} & ... & -2 \sum_{i}{x_{i_1} \theta_{k_1}}\\
\vdots & & \vdots \\
-2 \sum_{i}{x_{i_p} \theta_{1_p}} & ... & -2 \sum_{i}{x_{i_p} \theta_{k_p}}
 \end{bmatrix}
 = 0
$$



---

# Least squares demo

```{python}
import numpy as np
import matplotlib.pyplot as plt

dim = 2
N = 100

params = [2, 3]
interval = [0, 200]
outlier_fraction = 0.25

## create dataset

x = np.random.randint(interval[0], interval[1], (N, 2))
gt = x[:,0] * params[0] + x[:,1] * params[1]

# add random noise
y = gt + np.random.random(gt.shape)*0.1

# add outliers with 1000*N(0,1)
idx = np.random.randint(0, N, int(outlier_fraction * N))
y[idx] = y[idx] + np.random.rand(idx.size)*1000




# plot
xx, yy = np.meshgrid(range(interval[1]), range(interval[1]))
z_plane = xx*params[0] + yy*params[1]

# Create figure
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

# Plot 3D plane
ax.plot_surface(xx, yy, z_plane, alpha=0.8)

# Set labels
ax.set_xlabel('x1')
ax.set_ylabel('x2')
ax.set_zlabel('y')


#and i would like to plot this point : 
ax.scatter(x[:,0], x[:,1], gt, color='red')

ax.scatter(x[:,0], x[:,1], y, color='purple')

# when directly solving the linear equation X.theta = y, this is overdetermined, because there are many more equations than variables, so I'm just using the first #dim equations
# if we were doing RANSAC, the number of samples each time would also be this if we were going for the naive simple linear model

z_gt = np.linalg.solve(x[:dim, :dim],
                    gt[:dim])

print('z_gt=', z_gt)

z = np.linalg.solve(x.T.dot(x),
                    x.T.dot(y))

print('z=', z)

# plot estimated surface
# z_hat_plane = xx*z[0] + yy*z[1]
# ax.plot_surface(xx, yy, z_hat_plane, alpha=0.8)

# compute sum of squared error with z model
y_hat = x.dot(z)
for _y, _yhat in zip(y, y_hat):
    print(_y, _yhat, (_y - _yhat)**2)
sse = np.sum((y - y_hat)**2)**(1/2)
print('sse=', sse)


plt.show()

```

Example with wider estimation horizon window

```{python}
import numpy as np
import matplotlib.pyplot as plt

p = 2
k = 2
N = 100

params = np.array([
    [2, 3],
    [-5,6],
]).T

interval = [0, 200]
outlier_fraction = 0.25

## create dataset

x = np.random.randint(interval[0], interval[1], (N, 2))

gt = [sum([x[:,d]*params[h][d] for d in range(p)]) for h in range(k)]
gt = np.column_stack(gt)

# add random noise
y = gt + np.random.random(gt.shape)*0.1

# add outliers with 1000*N(0,1)
n_outliers = int(outlier_fraction * N)
idx = np.random.randint(0, N, n_outliers)
y[idx] = y[idx] + np.random.random((n_outliers, k))*1000



# plot
xx, yy = np.meshgrid(range(interval[1]), range(interval[1]))
z_planes = [xx*param[0] + yy*param[1] for param in params]

# Create figure
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

# Plot 3D plane
ax.plot_surface(xx, yy, z_planes[0], alpha=0.8)
ax.plot_surface(xx, yy, z_planes[1], alpha=0.8)

# Set labels
ax.set_xlabel('x1')
ax.set_ylabel('x2')
ax.set_zlabel('y')


#and i would like to plot this point : 
ax.scatter(x[:,0], x[:,1], gt[:,0], color='red')
ax.scatter(x[:,0], x[:,1], gt[:,1], color='red')

#ax.scatter(x[:,0], x[:,1], y, color='purple')

# when directly solving the linear equation X.theta = y, this is overdetermined, because there are many more equations than variables, so I'm just using the first #dim equations
# if we were doing RANSAC, the number of samples each time would also be this if we were going for the naive simple linear model

z_gt = np.linalg.solve(x[:p, :p],
                    gt[:p])

print('z_gt=', z_gt)

z = np.linalg.solve(x.T.dot(x),
                    x.T.dot(y))

print('z=', z)

# plot estimated surface
# z_hat_plane = xx*z[0] + yy*z[1]
# ax.plot_surface(xx, yy, z_hat_plane, alpha=0.8)

# compute sum of squared error with z model
y_hat = x.dot(z)
# for _y, _yhat in zip(y, y_hat):
#     print(_y, _yhat, (_y - _yhat)**2)
sse = np.sum((y - y_hat)**2)**(1/2)
print('sse=', sse)


plt.show()

```


# Ransac

Procedure
•choose a minimal set of data allowing to estimate the parameters
•compute the number of observations well approximated by the model
•repeat the previous steps N times and at the end choose the estimate with bigger support
(the estimate is refined using the support observations)KK


```{python}
'''
Consider two parabolas and observations close to each of them.
However, we do not know which parabola fits each observation.
Estimate the coefficients of the parabolas using the least squares
method and robust methods.
Characterize the performance of both methods
'''
import numpy as np
import matplotlib.pyplot as plt

# Generate some sample data points for a parabola
x = np.linspace(-10, 10, 100)
y_true = 2 * x**2 + 3 * x + 5  # True parabola equation

# Add some random noise to the data to make it realistic
noise = np.random.normal(0, 10, len(x))
y_data = y_true + noise

# Create the design matrix (X) and the target vector (y)
X = np.column_stack((x**2, x, np.ones(len(x))))  # x^2, x, and a bias term
y = y_data

# Use linear regression to fit a linear model
coefficients, residuals, _, _ = np.linalg.lstsq(X, y, rcond=None)

# Extract the coefficients for the linear model
a, b, c = coefficients

# Predicted values using the linear model
y_pred = a * x**2 + b * x + c

# Plot the original data and the linear regression model
plt.figure(figsize=(8, 6))
plt.scatter(x, y_data, label='Sample Data with Noise')
plt.plot(x, y_pred, 'r', label='Linear Regression Model')
plt.plot(x, y_true, 'g', label='True Parabola')
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.title('Linear Regression on a Parabola')
plt.grid(True)
plt.show()

# Print the coefficients of the linear model
print(f"Estimated coefficients: a = {a}, b = {b}, c = {c}")

```


```{python}
import numpy as np
import matplotlib.pyplot as plt

# Generate some sample data points for a quadratic relationship
x = np.linspace(-5, 5, 50)
y_true = 2 * x**2 + 3 * x + 5  # True quadratic equation

# Add some random noise to the data to make it realistic
noise = np.random.normal(0, 5, len(x))
y_data = y_true + noise

# Initialize the parameters for the quadratic model (y = ax^2 + bx + c)
a = 1  # Initial guess for the coefficient of x^2
b = 1  # Initial guess for the coefficient of x
c = 1  # Initial guess for the constant term

# Set hyperparameters for gradient descent
learning_rate = 0.01
num_iterations = 1000

# Perform gradient descent to optimize the parameters
for _ in range(num_iterations):
    # Calculate the predicted values using the current parameters
    y_pred = a * x**2 + b * x + c
    
    # Calculate the gradients with respect to a, b, and c
    gradient_a = (-2/len(x)) * np.sum(x**2 * (y_data - y_pred))
    gradient_b = (-2/len(x)) * np.sum(x * (y_data - y_pred))
    gradient_c = (-2/len(x)) * np.sum(y_data - y_pred)
    
    # Update the parameters using the gradients and learning rate
    a -= learning_rate * gradient_a
    b -= learning_rate * gradient_b
    c -= learning_rate * gradient_c

# Predicted values using the optimized parameters
y_pred = a * x**2 + b * x + c

# Plot the original data and the quadratic regression model
plt.figure(figsize=(8, 6))
plt.scatter(x, y_data, label='Sample Data with Noise')

# plt.plot(x, y_true, 'g', label='True Quadratic Relationship')
plt.plot(x, y_pred, 'r', label='Quadratic Regression Model')
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.title('Quadratic Regression Using Gradient Descent')
plt.grid(True)
plt.show()

# Print the optimized coefficients of the quadratic model
print(f"Optimized coefficient of x^2 (a): {a}")
print(f"Optimized coefficient of x (b): {b}")
print(f"Optimized constant term (c): {c}")

```


```{python}
import numpy as np
import matplotlib.pyplot as plt

# Generate some sample data points for a quadratic relationship
x = np.linspace(-20, 20, 200)

def parabola(x,a,b,c,shift=0):
    return a*(x-shift)**2 + b*(x-shift) + c

y_true = np.ones_like(x)
y_true[::2] = parabola(x[::2], 6, 4, 1, 0)# True quadratic equation

# another parabola shifted in x to the right
y_true[1::2] = parabola(x[1::2], 6, 1, 1, 10)

# Add some random noise to the data to make it realistic
y_data = y_true + np.random.normal(0, 50, y_true.size)

# another parabola shifter to the right


x = x.reshape((x.size,1))

xx = np.hstack([x**2, x, np.ones_like(x)])


coefs = np.linalg.pinv(xx).dot(y_true)

a,b,c = coefs

# Predicted values using the linear model
y_pred = a * x**2 + b * x + c

# residuals
residuals = y_pred - y_true



plt.figure(figsize=(8, 6))
plt.scatter(x, y_data, label='Sample Data with Noise')
plt.plot(x, y_pred, 'r', label='Linear Regression Model')
plt.plot(x, y_true, 'g', label='True Parabola')
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.title('Linear Regression on a Parabola')
plt.grid(True)
plt.show()

# Print the coefficients of the linear model
print(f"Estimated coefficients: a = {a}, b = {b}, c = {c}")

```

- A parabola has 3 parameters, so the minimum number of points we need in each RANSAC sample is 3.
- RANSAC is very sensitive to the threshold value. For 2 parabolas, if only changing x displacement and with sufficient noise, threshold must be set to a low value to find a solution close to the true parabolas.
- By running RANSAC multiple times, we consistly obain a good aproximation of each parabola.
- Interesting questions to answer:
    - what happens when there is more data available for one parabola? guess: the estimated model is only a good fit for the dominating parabola

```{python}
import numpy as np
import matplotlib.pyplot as plt
import random

# ransac
s = 3 # sample size
N = 50 # iterations
thresh = 3 # max distance to model



# data
var = 10 # noise variance
parabolas = [
    (6, 1, 1, 0),
    (6, 1, 1, 2),
    (6, 1, 1, 6),
]



# Generate some sample data points for a quadratic relationship
x = np.linspace(-5, 5, 100 * len(parabolas))

def parabola(x,a,b,c,shift=0):
    return a*(x-shift)**2 + b*(x-shift) + c

y_true = np.ones_like(x)

step_size = len(parabolas)
for i, (a,b,c,shift) in enumerate(parabolas):
    y_true[i::step_size] = parabola(x[i::step_size], a, b, c, shift)# True quadratic equation

# Add some random noise to the data to make it realistic
y_data = y_true + np.random.normal(0, var, y_true.size)

x = x.reshape((x.size,1))
xx = np.hstack([x**2, x, np.ones_like(x)])

# ransac
def fit_eval_model_sample(sample: list[int]):
    #coefs = np.linalg.pinv(xx[sample]).dot(y_data[sample])
    coefs = np.linalg.inv(xx[sample]).dot(y_data[sample])
    a,b,c = coefs
    y_pred = a * x**2 + b * x + c
    residuals = np.abs(y_pred.flatten() - y_data)
    inliners = np.arange(x.size)[residuals < thresh]
    return coefs, inliners, inliners.size
    

samples = [sorted(random.sample(range(x.size), s)) for _ in range(N)]

#models = [np.linalg.pinv(xx[sample]).dot(y_data[sample]) for sample in samples]
fits = [fit_eval_model_sample(sample) for sample in samples]
fits = sorted(fits, key=lambda x: x[-1])
_, inliners, _ = fits[-1] # best model

print(f" using {inliners.size} inliners")


coefs = np.linalg.pinv(xx[inliners]).dot(y_data[inliners])

a,b,c = coefs

# Predicted values using the linear model
y_pred = a * x**2 + b * x + c

# residuals
residuals = y_pred - y_true



plt.figure(figsize=(8, 6))
plt.scatter(x, y_data, label='Sample Data with Noise')
plt.plot(x, y_pred,  'black', label='RANSAC Model')
# plot true parabolas
for i, _ in enumerate(parabolas):
    plt.plot(x[i::len(parabolas)], y_true[i::len(parabolas)], label=f'True Parabola {i}')
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.title('Least Squares on a Parabola')
plt.grid(True)
plt.show()

# Print the coefficients of the linear model
print(f"Estimated coefficients: a = {a}, b = {b}, c = {c}")

```