---
title: L5 - Pattern Recognition
subtitle: Problem set
author: "Group 9 - Diogo Silva, SaÃºl Santos"
#date: "2023-10-22"
categories: [estimation-classification-course,phd,lecture-notes]
draft: false
format:
  html:
    code-fold: true
    embed-resources: true
  pdf: default
  latex: default
---


# Problem 4

## Problem statement

Determine the error probability of a MAP classifier knowing that y is generated by one of two classes with distribution

$$
\begin{cases}
    \alpha_i e^{-\alpha_i y} , y > 0 \\
    0, y \leq 0
\end{cases}
$$

The classes are equiprobable.

## Solution


The decision boundary is defined by the place where the posterior probability of both classes is equal.

The posterior probability is given by:

$$
P(i|y) = k P(y|i) P(i) = k \alpha_i e^{-\alpha_i y} P(i)
$$

Note that since the classes are equiprobable and both posteriors have the same normalizing constant, we're left just with $P(y|i)$ for finding the decision boundary:
$$
\alpha_1 e^{-\alpha_1 y} = \alpha_2 e^{-\alpha_2 y}
$$

Solving this w.r.t. $y$, we have the decision boundary at threshold $T$:

$$
y = \frac{ln(\frac{\alpha_2}{\alpha_1})}{\alpha_2 - \alpha_1} = T
$$

So, our classifier assigns classes in the following way.

$$
\begin{cases}
    y>T, i=1 \rightarrow \text{decision region } R_1 \\
    y<T, i=2 \rightarrow \text{decision region } R_2
\end{cases}
$$

From here, we can compute the confusion matrix $P$:

$$
P = \begin{bmatrix}
    p_{11} & p_{12} \\
    p_{21} & p_{22} \\
\end{bmatrix} = 
\begin{bmatrix}
    P(i=1|y, i=1) & P(i=2|y, i=1) \\
    P(i=1|y, i=2) & P(i=2|y, i=2) \\
\end{bmatrix}
$$

We'll compute correct classification (main diagonal), and deduce the misclassification probabilities from that, since each row sums to 1. We're assuming $\alpha_i>0$, otherwise we would get negative values in the p.d.f..

$$
p_{11} = \int_{R_1} p(i=1|y, i=1) dy = \int_{T}^{\infty} \alpha_1 e^{-\alpha_1 y} dy = -e^{-\alpha_1 y}|^{\infty}_{T} = e^{-\alpha_1 T}
$$

$$
p_{22} = \int_{R_2} p(i=2|y, i=2) dy = \int_{0}^{T} \alpha_2 e^{-\alpha_2 y} dy = -e^{-\alpha_2 y}|^{T}_{0} = 1 - e^{-\alpha_2 T}
$$

Total error probability $P_e$ (note both priors $P_i=0.5$, since we have equiprobable classes), is given by:

$$
\begin{cases}
P_e = 1 - \sum_{i=1}^2 p_{ii} P_i = 1 - 0.5 (e^{-\alpha_1 T} + 1 - e^{-\alpha_2 T})  \\
T = \frac{ln(\frac{\alpha_2}{\alpha_1})}{\alpha_2 - \alpha_1}
\end{cases}
$$



# Problem 2

```{python}
import numpy as np
import matplotlib.pyplot as plt
import math

x = np.linspace(-20,20,1000)
def pdf(x, mu, sig):
    return (1 / (np.sqrt(2 * math.pi * sig**2))) * np.exp(-0.5 * (x - mu)**2 / sig**2)

p1 = 0.2+1e-3
p2 = 1 - p1

#plt.plot(x, pdf(x, 0, 1), label='c1')
#plt.plot(x, pdf(x, 0, 4), label='c2')

y1 = pdf(x, 0, 1)*p1
y2 = pdf(x, 0, 4)*p2

print(f"where class 1 is higher than class 2:\n {np.where(y1 > y2)}")

plt.plot(x, pdf(x, 0, 1)*p1, label='c1')
plt.plot(x, pdf(x, 0, 4)*p2, label='c2')
plt.legend()
```