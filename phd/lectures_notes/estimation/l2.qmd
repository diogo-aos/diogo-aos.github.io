---
title: Lesson 2 - 
author: "Diogo Silva"
date: "2021-03-27"
categories: [estimation-classification-course,phd,lecture-notes]
draft: false
format:
  html:
    code-fold: true
---

# Overview

- Classic estimation
    - Parabolic fit
    - Least mean squares
    - Estimation
    - Regression
    - Frequency estimation
    - Robustness
    - RANSAC
    - Exercises & Work
- Classic probabilistic methods
    - classic vs bayesian
    - Maximum Likelihood
    - ML vs LS
    - Crámer-Rao bound
    - Monte-Carlo
    - Exercises
- Bibliography
    - Duda, Hart, Stork, Pattern Classification, Wiley, 2001.
    - J. Marques, Reconhecimento de Padrões. Métodos Estatísticos e Neuronais, IST Press, 1999


# Exercises

## Problem 1 - linear least squares, predict signal at t

Given a signal $y=( y_1 , ..., y_N )$, determine the coefficients of the linear predictor

$y_t = a_1 y_{t-1} +  ...  a_p y_{t-p}, N>>P$

using the least squares method.

---

Let $\theta = [a_1,..., a_p]^T$ be the parameter set.

Error is given by


$$
E = \sum_{i}{(y_i - x_i \theta )^2}
$$

Where, for each $y_i$, we have $x_i = (y_{i-1}, ..., y_{i-p})$. This means, using historical data, we can't use $y_1, ..., y_p$.

We want to minimize $\theta$ such that $E = 0$.

$$
\frac{\partial E}{\partial \theta_j} = \frac{\partial \sum_{i}{(y_i - x_i \theta )^2}}{\partial \theta_j} = 
2 \sum_{i}{ \frac{\partial(y_i - x_i \theta )}{\partial \theta_j}} = 
2 \sum_{i}{ -x_{i_j} \theta_j}
$$

We then have 

$$
\frac{\partial E}{\partial \theta} = 
\begin{bmatrix}
-2 \sum_{i}{x_{i_1} \theta_1} \\
\vdots \\
-2 \sum_{i}{x_{i_p} \theta_p} \\
 \end{bmatrix}
 = 0
$$

Solving this equation will give us the parameter values.


## Problem 2 - predict signal in [t,t+k] TODO RECHECK

By increasing the prediction window, we're increasing the number of parameters.
If, for each prediction, we were looking at $p$ previous signals and using $p$ parameters for prediciting the next signal $y_t$, now we'll need $k \times p$ parameters for predicting $(y_t, ..., y_{t+k})$. We have 



$$
\begin{bmatrix}
x_1 \\
\vdots \\
x_N
\end{bmatrix}
\times
\begin{bmatrix}
a_{1_1}  & ... & a_{k_1} \\
\vdots &  & \vdots \\
a_{1_p}  & ... & a_{k_p} \\
\end{bmatrix} = 
\begin{bmatrix}
y_{1_1} & ... & y_{1_k} \\
\vdots &  & \vdots \\
y_{N_1}  & ... & y_{N_k} \\
\end{bmatrix}
$$

Since the prediciton at $y_t$ doesn't affect subsequent predictions and the parameters for $y_t$

$$
\frac{\partial E}{\partial \theta} = 
\begin{bmatrix}
-2 \sum_{i}{x_{i_1} \theta_{1_1}} & ... & -2 \sum_{i}{x_{i_1} \theta_{k_1}}\\
\vdots & & \vdots \\
-2 \sum_{i}{x_{i_p} \theta_{1_p}} & ... & -2 \sum_{i}{x_{i_p} \theta_{k_p}}
 \end{bmatrix}
 = 0
$$



---

# Least squares demo

```{python}
import numpy as np
import matplotlib.pyplot as plt

dim = 2
N = 100

params = [2, 3]
interval = [0, 200]
outlier_fraction = 0.25

## create dataset

x = np.random.randint(interval[0], interval[1], (N, 2))
gt = x[:,0] * params[0] + x[:,1] * params[1]

# add random noise
y = gt + np.random.random(gt.shape)*0.1

# add outliers with 1000*N(0,1)
idx = np.random.randint(0, N, int(outlier_fraction * N))
y[idx] = y[idx] + np.random.rand(idx.size)*1000




# plot
xx, yy = np.meshgrid(range(interval[1]), range(interval[1]))
z_plane = xx*params[0] + yy*params[1]

# Create figure
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

# Plot 3D plane
ax.plot_surface(xx, yy, z_plane, alpha=0.8)

# Set labels
ax.set_xlabel('x1')
ax.set_ylabel('x2')
ax.set_zlabel('y')


#and i would like to plot this point : 
ax.scatter(x[:,0], x[:,1], gt, color='red')

ax.scatter(x[:,0], x[:,1], y, color='purple')

# when directly solving the linear equation X.theta = y, this is overdetermined, because there are many more equations than variables, so I'm just using the first #dim equations
# if we were doing RANSAC, the number of samples each time would also be this if we were going for the naive simple linear model

z_gt = np.linalg.solve(x[:dim, :dim],
                    gt[:dim])

print('z_gt=', z_gt)

z = np.linalg.solve(x.T.dot(x),
                    x.T.dot(y))

print('z=', z)

# plot estimated surface
# z_hat_plane = xx*z[0] + yy*z[1]
# ax.plot_surface(xx, yy, z_hat_plane, alpha=0.8)

# compute sum of squared error with z model
y_hat = x.dot(z)
for _y, _yhat in zip(y, y_hat):
    print(_y, _yhat, (_y - _yhat)**2)
sse = np.sum((y - y_hat)**2)**(1/2)
print('sse=', sse)


plt.show()

```